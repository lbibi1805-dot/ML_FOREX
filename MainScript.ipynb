{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 0: IMPORT LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import OneHotEncoder      \n",
    "from sklearn.model_selection import KFold   \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib \n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# Additional useful imports\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "import yfinance as yf\n",
    "import ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forex Data Analysis and Prediction\n",
    "\n",
    "This script is designed to perform comprehensive data analysis and predictive modeling on Forex data. It leverages historical data and technical indicators to forecast Forex prices using various machine learning models.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Requirements](#requirements)\n",
    "2. [Installation](#installation)\n",
    "3. [Usage](#usage)\n",
    "4. [Features](#features)\n",
    "5. [Models](#models)\n",
    "6. [Output](#output)\n",
    "7. [Evaluation and Fine-Tuning](#evaluation-and-fine-tuning)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.x\n",
    "- Libraries: \n",
    "  - pandas\n",
    "  - numpy\n",
    "  - matplotlib\n",
    "  - seaborn\n",
    "  - scikit-learn\n",
    "  - lightgbm\n",
    "  - xgboost\n",
    "  - yfinance\n",
    "  - ta (Technical Analysis Library in Python)\n",
    "  - joblib\n",
    "\n",
    "## Installation\n",
    "\n",
    "1. Clone the repository:\n",
    "   ```bash\n",
    "   git clone https://github.com/yourusername/forex-prediction.git\n",
    "   cd forex-prediction\n",
    "Install the required libraries:\n",
    "Sao chép\n",
    "pip install -r requirements.txt\n",
    "Usage\n",
    "Run the script to perform data crawling, feature engineering, and model training:\n",
    "\n",
    "Sao chép\n",
    "python forex_prediction.py\n",
    "The script will output the results of various models and save trained models in the models directory.\n",
    "\n",
    "Features\n",
    "Data Crawling\n",
    "Yahoo Finance API: Utilizes the yfinance library to download historical Forex data.\n",
    "Data Storage: Saves the fetched data into CSV files for further processing.\n",
    "Feature Engineering\n",
    "Technical Indicators: Computes a range of technical indicators to enhance the predictive power of models:\n",
    "Moving Averages (MA): Calculates 10, 50, and 200-day moving averages to identify trends.\n",
    "Relative Strength Index (RSI): Measures the speed and change of price movements.\n",
    "MACD (Moving Average Convergence Divergence): Used to identify potential buy and sell signals.\n",
    "Bollinger Bands: Helps identify overbought or oversold conditions in the market.\n",
    "Average True Range (ATR): Indicates market volatility.\n",
    "And many more: Including ADX, Aroon, Stochastic Oscillator, etc.\n",
    "Data Preprocessing\n",
    "Handling Missing Values: Uses interpolation and other imputation methods to fill missing data points.\n",
    "Normalization and Scaling: Standardizes features to ensure that they contribute equally to the model performance.\n",
    "Models\n",
    "The script implements and evaluates several machine learning models:\n",
    "\n",
    "LightGBM Regressor: A gradient boosting framework that uses tree-based learning algorithms, known for its speed and efficiency.\n",
    "\n",
    "XGBoost Regressor: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable.\n",
    "\n",
    "Decision Tree Regressor: A non-parametric supervised learning method used for regression that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "Polynomial Regression: Extends linear regression by considering polynomial features, allowing the model to fit a wider range of curves.\n",
    "\n",
    "Linear Regression: A basic but powerful model that assumes a linear relationship between input features and the target variable.\n",
    "\n",
    "Random Forest Regressor: An ensemble method that builds multiple decision trees and merges them to get a more accurate and stable prediction.\n",
    "\n",
    "K-Nearest Neighbors Regressor: A non-parametric method that predicts the target by averaging the values of the k-nearest neighbors.\n",
    "\n",
    "Support Vector Regressor (SVR): Uses the principles of support vector machines for regression challenges, effective in high-dimensional spaces.\n",
    "\n",
    "Output\n",
    "Predicted Forex Prices: The primary output of the script is the predicted closing prices of the specified Forex pair(s). These predictions are generated based on the historical data and technical indicators processed by the models.\n",
    "Evaluation Metrics: The script also provides evaluation metrics such as R2 score and RMSE to assess the accuracy and reliability of the predictions.\n",
    "Evaluation and Fine-Tuning\n",
    "Evaluation\n",
    "Cross-Validation: Uses K-Fold cross-validation to assess the robustness of the models. It splits the dataset into k different subsets and trains the model k times, each time using a different subset as the test set.\n",
    "Residual Analysis: Plots residual distributions to visually assess the performance and fit of the models.\n",
    "Fine-Tuning\n",
    "Grid Search: Utilizes GridSearchCV to perform hyperparameter tuning, systematically building and evaluating models for each combination of algorithm parameters specified in a grid.\n",
    "Performance Metrics: Evaluates models using R2 score and RMSE to quantify the accuracy of predictions.\n",
    "Notes\n",
    "Ensure that the dataset directory and model storage paths are correctly set up in the script.\n",
    "Modify the forex_label list to include additional Forex pairs if needed.\n",
    "Sao chép\n",
    "\n",
    "This Markdown cell can be copied into a Jupyter Notebook or any Markdown editor to provide a comprehensive overview of the script's functionality.\n",
    "nhồi nó vào 1 cell\n",
    "\n",
    "GPT-4o\n",
    "Sure! Here is the entire README content formatted as a single Markdown cell:\n",
    "\n",
    "Sao chép\n",
    "# Forex Data Analysis and Prediction\n",
    "\n",
    "This script is designed to perform comprehensive data analysis and predictive modeling on Forex data. It leverages historical data and technical indicators to forecast Forex prices using various machine learning models.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Requirements](#requirements)\n",
    "2. [Installation](#installation)\n",
    "3. [Usage](#usage)\n",
    "4. [Features](#features)\n",
    "5. [Models](#models)\n",
    "6. [Output](#output)\n",
    "7. [Evaluation and Fine-Tuning](#evaluation-and-fine-tuning)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.x\n",
    "- Libraries: \n",
    "  - pandas\n",
    "  - numpy\n",
    "  - matplotlib\n",
    "  - seaborn\n",
    "  - scikit-learn\n",
    "  - lightgbm\n",
    "  - xgboost\n",
    "  - yfinance\n",
    "  - ta (Technical Analysis Library in Python)\n",
    "  - joblib\n",
    "\n",
    "## Installation\n",
    "\n",
    "1. Clone the repository:\n",
    "   ```bash\n",
    "   git clone https://github.com/yourusername/forex-prediction.git\n",
    "   cd forex-prediction\n",
    "Install the required libraries:\n",
    "Sao chép\n",
    "pip install -r requirements.txt\n",
    "Usage\n",
    "Run the script to perform data crawling, feature engineering, and model training:\n",
    "\n",
    "Sao chép\n",
    "python forex_prediction.py\n",
    "The script will output the results of various models and save trained models in the models directory.\n",
    "\n",
    "Features\n",
    "Data Crawling\n",
    "Yahoo Finance API: Utilizes the yfinance library to download historical Forex data.\n",
    "Data Storage: Saves the fetched data into CSV files for further processing.\n",
    "Feature Engineering\n",
    "Technical Indicators: Computes a range of technical indicators to enhance the predictive power of models:\n",
    "Moving Averages (MA): Calculates 10, 50, and 200-day moving averages to identify trends.\n",
    "Relative Strength Index (RSI): Measures the speed and change of price movements.\n",
    "MACD (Moving Average Convergence Divergence): Used to identify potential buy and sell signals.\n",
    "Bollinger Bands: Helps identify overbought or oversold conditions in the market.\n",
    "Average True Range (ATR): Indicates market volatility.\n",
    "And many more: Including ADX, Aroon, Stochastic Oscillator, etc.\n",
    "Data Preprocessing\n",
    "Handling Missing Values: Uses interpolation and other imputation methods to fill missing data points.\n",
    "Normalization and Scaling: Standardizes features to ensure that they contribute equally to the model performance.\n",
    "Models\n",
    "The script implements and evaluates several machine learning models:\n",
    "\n",
    "LightGBM Regressor: A gradient boosting framework that uses tree-based learning algorithms, known for its speed and efficiency.\n",
    "\n",
    "XGBoost Regressor: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable.\n",
    "\n",
    "Decision Tree Regressor: A non-parametric supervised learning method used for regression that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "Polynomial Regression: Extends linear regression by considering polynomial features, allowing the model to fit a wider range of curves.\n",
    "\n",
    "Linear Regression: A basic but powerful model that assumes a linear relationship between input features and the target variable.\n",
    "\n",
    "Random Forest Regressor: An ensemble method that builds multiple decision trees and merges them to get a more accurate and stable prediction.\n",
    "\n",
    "K-Nearest Neighbors Regressor: A non-parametric method that predicts the target by averaging the values of the k-nearest neighbors.\n",
    "\n",
    "Support Vector Regressor (SVR): Uses the principles of support vector machines for regression challenges, effective in high-dimensional spaces.\n",
    "\n",
    "Output\n",
    "Predicted Forex Prices: The primary output of the script is the predicted closing prices of the specified Forex pair(s). These predictions are generated based on the historical data and technical indicators processed by the models.\n",
    "Evaluation Metrics: The script also provides evaluation metrics such as R2 score and RMSE to assess the accuracy and reliability of the predictions.\n",
    "Evaluation and Fine-Tuning\n",
    "Evaluation\n",
    "Cross-Validation: Uses K-Fold cross-validation to assess the robustness of the models. It splits the dataset into k different subsets and trains the model k times, each time using a different subset as the test set.\n",
    "Residual Analysis: Plots residual distributions to visually assess the performance and fit of the models.\n",
    "Fine-Tuning\n",
    "Grid Search: Utilizes GridSearchCV to perform hyperparameter tuning, systematically building and evaluating models for each combination of algorithm parameters specified in a grid.\n",
    "Performance Metrics: Evaluates models using R2 score and RMSE to quantify the accuracy of predictions.\n",
    "Notes\n",
    "Ensure that the dataset directory and model storage paths are correctly set up in the script.\n",
    "Modify the forex_label list to include additional Forex pairs if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 1: DEFINE LABEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forex_label = ['GBPJPY=X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 2: GET FOREX DATA CRAWLING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1: Crawl all FOREX data needed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.1. Function to crawl Forex data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_forex_data(forex_pair, directory='Dataset'):\n",
    "    \"\"\"\n",
    "    Crawls historical data for a given Forex pair and saves it to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    forex_pair (str): The Forex pair symbol to download data for (e.g., 'EURUSD=X').\n",
    "    directory (str): The directory where the data should be saved.\n",
    "\n",
    "    Returns:\n",
    "    str: Path to the saved CSV file.\n",
    "    pd.DataFrame: The crawled data as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Download historical data for the Forex pair\n",
    "    print(f\"Downloading data for {forex_pair}...\")\n",
    "    data = yf.download(forex_pair, period=\"max\", interval='1d')\n",
    "    data.reset_index(inplace=True)  # Reset index to ensure 'Date' is a normal column\n",
    "\n",
    "    # Save data to CSV\n",
    "    file_path = os.path.join(directory, f'{forex_pair}_data.csv')\n",
    "    data.to_csv(file_path, index=False)\n",
    "    print(f\"Saved data for {forex_pair} at {file_path}\")\n",
    "\n",
    "    return file_path, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2. Check crawling result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for ['GBPJPY=X']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for ['GBPJPY=X'] at Dataset\\['GBPJPY=X']_data.csv\n",
      "        Date        Open        High         Low       Close   Adj Close  \\\n",
      "0 2003-12-01  189.330002  189.660004  187.740005  187.630005  187.630005   \n",
      "1 2003-12-02  187.669998  188.809998  187.559998  188.009995  188.009995   \n",
      "2 2003-12-03  188.020004  188.240005  186.490005  187.089996  187.089996   \n",
      "3 2003-12-04  187.029999  187.029999  185.899994  186.220001  186.220001   \n",
      "4 2003-12-05  186.190002  186.740005  185.830002  185.880005  185.880005   \n",
      "\n",
      "   Volume  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5427 entries, 0 to 5426\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Date       5427 non-null   datetime64[ns]\n",
      " 1   Open       5427 non-null   float64       \n",
      " 2   High       5427 non-null   float64       \n",
      " 3   Low        5427 non-null   float64       \n",
      " 4   Close      5427 non-null   float64       \n",
      " 5   Adj Close  5427 non-null   float64       \n",
      " 6   Volume     5427 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
      "memory usage: 296.9 KB\n",
      "None\n",
      "\n",
      "Statistics of NaN values in the DataFrame:\n",
      "Date         0\n",
      "Open         0\n",
      "High         0\n",
      "Low          0\n",
      "Close        0\n",
      "Adj Close    0\n",
      "Volume       0\n",
      "dtype: int64\n",
      "\n",
      "Total number of NaN values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path, data = crawl_forex_data(forex_label)\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "print(data.head(5))\n",
    "print(data.info())\n",
    "\n",
    "# Calculate and print statistics of NaN values\n",
    "nan_stats = data.isna().sum()\n",
    "total_nans = nan_stats.sum()\n",
    "\n",
    "print(\"\\nStatistics of NaN values in the DataFrame:\")\n",
    "print(nan_stats)\n",
    "print(f\"\\nTotal number of NaN values: {total_nans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3: FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "# Calculate moving averages\n",
    "if 'Close' in data.columns:\n",
    "    if len(data) >= 10: data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "    if len(data) >= 50: data['MA_50'] = data['Close'].rolling(window=50).mean()\n",
    "    if len(data) >= 200: data['MA_200'] = data['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "data['RSI'] = ta.momentum.RSIIndicator(data['Close'], window=14).rsi()\n",
    "\n",
    "# Calculate MACD\n",
    "data['MACD'] = ta.trend.MACD(data['Close']).macd()\n",
    "\n",
    "# Calculate Bollinger Bands\n",
    "bollinger = ta.volatility.BollingerBands(data['Close'])\n",
    "data['BB_High'] = bollinger.bollinger_hband()\n",
    "data['BB_Low'] = bollinger.bollinger_lband()\n",
    "\n",
    "# Calculate Bollinger BandWidth (BBTrend)\n",
    "data['BB_Width'] = (data['BB_High'] - data['BB_Low']) / data['Close']\n",
    "\n",
    "# 24-hour Volume\n",
    "data['Volume_24h'] = data['Volume'].rolling(window=1440).sum()\n",
    "\n",
    "# Accumulation/Distribution Line (ADL)\n",
    "data['ADL'] = ta.volume.AccDistIndexIndicator(data['High'], data['Low'], data['Close'], data['Volume']).acc_dist_index()\n",
    "\n",
    "# Aroon Indicator\n",
    "aroon = ta.trend.AroonIndicator(data['Close'], data['Low'], window=25)\n",
    "data['Aroon_Up'] = aroon.aroon_up()\n",
    "data['Aroon_Down'] = aroon.aroon_down()\n",
    "\n",
    "# Average Directional Index (ADX)\n",
    "data['ADX'] = ta.trend.ADXIndicator(data['High'], data['Low'], data['Close'], window=14).adx()\n",
    "\n",
    "# Average True Range (ATR)\n",
    "data['ATR'] = ta.volatility.AverageTrueRange(data['High'], data['Low'], data['Close'], window=14).average_true_range()\n",
    "\n",
    "# Awesome Oscillator (AO)\n",
    "data['AO'] = ta.momentum.AwesomeOscillatorIndicator(data['High'], data['Low'], window1=5, window2=34).awesome_oscillator()\n",
    "\n",
    "# Balance of Power (BOP)\n",
    "data['BOP'] = (data['Close'] - data['Open']) / (data['High'] - data['Low'])\n",
    "\n",
    "# Bull Bear Power\n",
    "data['Bull_Power'] = data['High'] - data['MA_50']\n",
    "data['Bear_Power'] = data['Low'] - data['MA_50']\n",
    "\n",
    "# Chaikin Oscillator\n",
    "data['Chaikin_Osc'] = data['ADL'].ewm(span=3).mean() - data['ADL'].ewm(span=10).mean()\n",
    "\n",
    "# Stochastic Oscillator (Stoch)\n",
    "stoch = ta.momentum.StochasticOscillator(data['High'], data['Low'], data['Close'], window=14)\n",
    "data['Stoch_Osc'] = stoch.stoch()\n",
    "\n",
    "# Commodity Channel Index (CCI)\n",
    "data['CCI'] = ta.trend.CCIIndicator(data['High'], data['Low'], data['Close'], window=20).cci()\n",
    "\n",
    "# Vortex Oscillator\n",
    "vortex = ta.trend.VortexIndicator(data['High'], data['Low'], data['Close'], window=14)\n",
    "data['Vortex_Plus'] = vortex.vortex_indicator_pos()\n",
    "data['Vortex_Minus'] = vortex.vortex_indicator_neg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2. Check NaN values before interpolation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_stats_before = data.isna().sum()\n",
    "total_nans_before = nan_stats_before.sum()\n",
    "print(\"\\nStatistics of NaN values before interpolation:\")\n",
    "print(nan_stats_before)\n",
    "print(f\"Total number of NaN values before interpolation: {total_nans_before}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3. Use Interpolation to fill missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use Interpolation ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing data\n",
    "data.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows remain after interpolation\n",
    "nan_stats_after = data.isna().sum()\n",
    "total_nans_after = nan_stats_after.sum()\n",
    "print(\"\\nStatistics of NaN values after interpolation:\")\n",
    "print(f\"Total number of NaN values after interpolation: {total_nans_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: As we can see, after using the interpolation, there are still many NaN values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4: Detect NaN values left and fill them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are still NaN values, use another method to fill them\n",
    "if total_nans_after > 0:\n",
    "    # Fill remaining NaN values with forward fill\n",
    "    data.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # If still NaN, use backward fill\n",
    "    if data.isna().sum().sum() > 0:\n",
    "        data.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5. Examine for left over NaN result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check on NaN values\n",
    "nan_stats_final = data.isna().sum()\n",
    "total_nans_final = nan_stats_final.sum()\n",
    "print(\"\\n----Statistics of NaN values after final filling----\")\n",
    "print(f\"Total number of NaN values after final filling: {total_nans_final}\")\n",
    "\n",
    "# Check how many rows remain after dropping NaN values\n",
    "data.dropna(subset=['Close', 'Open'], inplace=True)\n",
    "print(f\"Number of rows after dropna: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6. Define list of indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of indicators to plot\n",
    "indicators = [\n",
    "    'MA_10', 'MA_50', 'MA_200',\n",
    "    'RSI', 'MACD',\n",
    "    'BB_High', 'BB_Low',\n",
    "    'ADX', 'ATR', 'AO',\n",
    "    'BOP', 'Bull_Power', 'Bear_Power',\n",
    "    'Chaikin_Osc',\n",
    "    'Stoch_Osc', 'CCI',\n",
    "    'Vortex_Plus', 'Vortex_Minus'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4: PLOTTING FOR EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to plot year data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_yearly_data(data, title='Yearly Price Data'):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(data['Date'], data['Close'], label='Close Price', color='blue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to plot prices vs indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prices_vs_indicators(data, indicator, title):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(data['Date'], data['Close'], label='Close Price', color='blue')\n",
    "    plt.plot(data['Date'], data[indicator], label=indicator, color='orange')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price / Indicator Value')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to plot out Label Price vs other forex pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Function to plot out Label Price vs Other Forex Pairs**\n",
    "def plot_label_vs_other_forex(data, label, other_pairs):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Check if the label exists in the DataFrame\n",
    "    if label not in data.columns:\n",
    "        print(f\"Label '{label}' not found in DataFrame columns: {data.columns}\")\n",
    "        return\n",
    "\n",
    "    plt.plot(data['Date'], data[label], label=label, color='blue')\n",
    "    \n",
    "    for pair in other_pairs:\n",
    "        pair_close = f'{pair}_Close'\n",
    "        if pair_close in data.columns:  # Ensure the pair exists in the DataFrame\n",
    "            plt.plot(data['Date'], data[pair_close], label=f'{pair} Close', linestyle='--')\n",
    "    \n",
    "    plt.title(f'{label} vs Other Forex Pairs')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1. Plot out yearly data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot yearly data\n",
    "plot_yearly_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4..2. Plot out indicators vs price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Close Price vs each indicator\n",
    "for indicator in indicators:\n",
    "    if indicator in data.columns:  # Check if the indicator exists\n",
    "        plot_prices_vs_indicators(data, indicator, f'Close Price vs {indicator}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 5 FIND UN-RELATED FEATURES USING MUTUAL INFORMATION (MI)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1. Calculate MI and decide which feature has the highest correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variables\n",
    "targets = ['Close', 'Open', 'High', 'Low']  # Add any other targets you want to consider\n",
    "\n",
    "# Initialize a DataFrame to hold MI scores for each target\n",
    "mi_results = pd.DataFrame()\n",
    "\n",
    "# Separate features\n",
    "X = data.drop(columns=['Date'] + targets)  # Exclude the target and date columns\n",
    "\n",
    "# Calculate Mutual Information for each target variable\n",
    "for target in targets:\n",
    "    y = data[target]\n",
    "    \n",
    "    # Calculate Mutual Information\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    \n",
    "    # Create a DataFrame for the current target's MI scores\n",
    "    mi_df = pd.DataFrame(mi_scores, index=X.columns, columns=[f'MI Score_{target}'])\n",
    "    \n",
    "    # Append to the results DataFrame\n",
    "    mi_results = pd.concat([mi_results, mi_df], axis=1)\n",
    "\n",
    "# Set display options to avoid scientific notation\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "# Display the MI scores for all targets\n",
    "print(\"Mutual Information Scores for each target:\")\n",
    "print(mi_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2. Drop unrelated features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for MI score\n",
    "mi_threshold = 0.25\n",
    "\n",
    "# Identify features to drop based on the MI score for each target\n",
    "features_to_drop = set()\n",
    "for target in targets:\n",
    "    low_mi_features = mi_results[mi_results[f'MI Score_{target}'] < mi_threshold].index.tolist()\n",
    "    features_to_drop.update(low_mi_features)\n",
    "\n",
    "# Drop the unrelated features from the DataFrame\n",
    "X_reduced = X.drop(columns=list(features_to_drop))\n",
    "\n",
    "# Optionally, update the DataFrame with the target\n",
    "data_reduced = data.drop(columns=list(features_to_drop))\n",
    "\n",
    "# Check the remaining features\n",
    "print(\"Remaining features after dropping unrelated ones:\")\n",
    "print(X_reduced.columns)\n",
    "\n",
    "# Save the reduced DataFrame if needed\n",
    "reduced_file_path = os.path.join('Dataset', forex_label[0] + '_reduced_data.csv')\n",
    "data_reduced.to_csv(reduced_file_path, index=False)\n",
    "print(f\"Saved reduced data to: {reduced_file_path}\")\n",
    "print(data.head(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 6: PREPARE AND TRANSFORM DATA-FRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def fit(self, dataframe, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        return dataframe[self.feature_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1. Shift Label for future predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "for target_column in targets:  # Iterate over each target\n",
    "    data[target_column] = data[target_column].shift(k)\n",
    "\n",
    "# Drop rows with NaN values created by the shift\n",
    "data = data.dropna()\n",
    "\n",
    "# Print some data \n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2. Split the data into training and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print out some information about the split\n",
    "print('\\n____________ Split training and test set ____________')     \n",
    "print(len(train_set), \"training +\", len(test_set), \"test examples\")\n",
    "print(train_set.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3. Define features and targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = data.columns.difference(['Date'] + targets).tolist()  # Define features excluding Date and targets\n",
    "X_reduced = data.drop(columns=['Date'] + targets).drop(columns=list(features_to_drop))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set[X_reduced.columns]\n",
    "X_test = test_set[X_reduced.columns]\n",
    "\n",
    "# Split targets for each label\n",
    "y_train_dict = {target: train_set[target] for target in targets}\n",
    "y_test_dict = {target: test_set[target] for target in targets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.4. Define and Train Model Pipelines for Each Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Redefine num_feat_names based on X_reduced\n",
    "num_feat_names = X_reduced.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(steps=[\n",
    "    ('selector', ColumnSelector(num_feat_names)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.4.2. Fit and Transform the Pipeline on training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the pipeline to process training data\n",
    "processed_train_set_val = num_pipeline.fit_transform(X_train)\n",
    "\n",
    "# Fit the pipeline on training data and transform both training and test data\n",
    "X_train = num_pipeline.fit_transform(X_train)\n",
    "X_test = num_pipeline.transform(X_test)\n",
    "\n",
    "# Convert the transformed data back to DataFrame with appropriate column names\n",
    "X_train = pd.DataFrame(X_train, columns=num_feat_names)\n",
    "X_test = pd.DataFrame(X_test, columns=num_feat_names)\n",
    "\n",
    "print('\\n____________ Processed feature values ____________')\n",
    "print(processed_train_set_val[:3, :])  # Print out some of the first rows of the training dataset after fit_transforming\n",
    "print(processed_train_set_val.shape)  # Print out the statistics of the training set\n",
    "\n",
    "# Save the numerical pipeline\n",
    "joblib.dump(num_pipeline, r'models/num_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 7. TRAIN AND EVALUATE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to calculate R2 score and Root Mean Squared Error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2score_and_rmse(model, train_data, labels): \n",
    "    r2score = model.score(train_data, labels)\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    prediction = model.predict(train_data)\n",
    "    mse = mean_squared_error(labels, prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return r2score, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Store and Load Models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_model(model, model_name = \"\"):\n",
    "    # NOTE: sklearn.joblib faster than pickle of Python\n",
    "    # INFO: can store only ONE object in a file\n",
    "    if model_name == \"\": \n",
    "        model_name = type(model).__name__\n",
    "    joblib.dump(model,'models/' + model_name + '_model.pkl')\n",
    "    print(f\"Model successfully saved as \" + model_name + '_model.pkl')\n",
    "    \n",
    "def load_model(model_name):\n",
    "    # Load objects into memory\n",
    "    #del model\n",
    "    model = joblib.load('models/' + model_name + '_model.pkl')\n",
    "    #print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.1. Try Light GBM model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    model = lgb.LGBMRegressor()  # Initialize the model\n",
    "    model.fit(X_train, y_train_dict[target])  # Fit the model on the respective target\n",
    "\n",
    "    print(f'\\n____________ LGBMRegressor for {target} ____________')\n",
    "\n",
    "    r2score, rmse = r2score_and_rmse(model, X_train, y_train_dict[target])\n",
    "    print('\\nR2 score (on training data, best=1):', r2score)\n",
    "    print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "    # Predict labels for some test instances\n",
    "    print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "    print(\"Labels:      \", list(y_test_dict[target][:9]))\n",
    "\n",
    "    store_model(model)  # Store the model for the current target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2. Try XGBoost model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    model = XGBRegressor() # Initialize the model\n",
    "    model.fit(X_train, y_train_dict[target])  # Fit the model on the respective target\n",
    "\n",
    "    print(f'\\n____________ XGBoost_Regressor for {target} ____________')\n",
    "\n",
    "    r2score, rmse = r2score_and_rmse(model, X_train, y_train_dict[target])\n",
    "    print('\\nR2 score (on training data, best=1):', r2score)\n",
    "    print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "    # Predict labels for some test instances\n",
    "    print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "    print(\"Labels:      \", list(y_test_dict[target][:9]))\n",
    "\n",
    "    store_model(model)  # Store the model for the current target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3. Try Decision Tree (in-lecture).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    model = DecisionTreeRegressor() # Initialize the model\n",
    "    model.fit(X_train, y_train_dict[target])  # Fit the model on the respective target\n",
    "\n",
    "    print(f'\\n____________ DecisionTreeRegressor for {target} ____________')\n",
    "\n",
    "    r2score, rmse = r2score_and_rmse(model, X_train, y_train_dict[target])\n",
    "    print('\\nR2 score (on training data, best=1):', r2score)\n",
    "    print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "    # Predict labels for some test instances\n",
    "    print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "    print(\"Labels:      \", list(y_test_dict[target][:9]))\n",
    "\n",
    "    store_model(model)  # Store the model for the current target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4. Try Polynomial Regression (in-lecture).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    degree = 2  # Degree of the polynomial\n",
    "    model = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(degree=degree)),\n",
    "        ('lin_reg', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    # Fit the model on the respective target\n",
    "    model.fit(X_train, y_train_dict[target])\n",
    "\n",
    "    print(f'\\n____________ PolynomialRegressor for {target} ____________')\n",
    "\n",
    "    r2score, rmse = r2score_and_rmse(model, X_train, y_train_dict[target])\n",
    "    print('\\nR2 score (on training data, best=1):', r2score)\n",
    "    print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "    # Predict labels for some test instances\n",
    "    predictions = model.predict(X_test[:9]).round(decimals=1)\n",
    "    print(\"\\nPredictions: \", predictions)\n",
    "    print(\"Labels:      \", list(y_test_dict[target][:9]))\n",
    "\n",
    "    store_model(model)  # Store the model for the current target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5. Try Linear Regressor (in-lecture).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    model = LinearRegression() # Initialize the model\n",
    "    model.fit(X_train, y_train_dict[target])  # Fit the model on the respective target\n",
    "\n",
    "    print(f'\\n____________ Linear Regressor for {target} ____________')\n",
    "\n",
    "    r2score, rmse = r2score_and_rmse(model, X_train, y_train_dict[target])\n",
    "    print('\\nR2 score (on training data, best=1):', r2score)\n",
    "    print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "    # Predict labels for some test instances\n",
    "    print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "    print(\"Labels:      \", list(y_test_dict[target][:9]))\n",
    "\n",
    "    store_model(model)  # Store the model for the current target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.6. Try Random Forest (in-lecture).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    model = RandomForestRegressor(n_estimators=20) # Initialize the model\n",
    "    model.fit(X_train, y_train_dict[target])  # Fit the model on the respective target\n",
    "\n",
    "    print(f'\\n____________ RandomForestRegressor for {target} ____________')\n",
    "\n",
    "    r2score, rmse = r2score_and_rmse(model, X_train, y_train_dict[target])\n",
    "    print('\\nR2 score (on training data, best=1):', r2score)\n",
    "    print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "    # Predict labels for some test instances\n",
    "    print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "    print(\"Labels:      \", list(y_test_dict[target][:9]))\n",
    "\n",
    "    store_model(model)  # Store the model for the current target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.7. Try K-Nearest-Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    model = KNeighborsRegressor(n_neighbors=5) # Initialize the model\n",
    "    model.fit(X_train, y_train_dict[target])  # Fit the model on the respective target\n",
    "\n",
    "    print(f'\\n____________ KNeighborRegressor for {target} ____________')\n",
    "\n",
    "    r2score, rmse = r2score_and_rmse(model, X_train, y_train_dict[target])\n",
    "    print('\\nR2 score (on training data, best=1):', r2score)\n",
    "    print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "    # Predict labels for some test instances\n",
    "    print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "    print(\"Labels:      \", list(y_test_dict[target][:9]))\n",
    "\n",
    "    store_model(model)  # Store the model for the current target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8. Try using SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    model = SVR(kernel='rbf')  # Initialize the model\n",
    "    model.fit(X_train, y_train_dict[target])  # Fit the model on the respective target\n",
    "\n",
    "    print(f'\\n____________ Support Vector Regressor (SVR) for {target} ____________')\n",
    "\n",
    "    r2score, rmse = r2score_and_rmse(model, X_train, y_train_dict[target])\n",
    "    print('\\nR2 score (on training data, best=1):', r2score)\n",
    "    print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "    # Predict labels for some test instances\n",
    "    print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "    print(\"Labels:      \", list(y_test_dict[target][:9]))\n",
    "\n",
    "    store_model(model)  # Store the model for the current target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 8: EVALUATE WITH K-CROSS VALIDATION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directories exist\n",
    "os.makedirs('saved_objects', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print('\\n____________ K-fold cross validation ____________')\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=37)  # cv data generator\n",
    "\n",
    "run_new_evaluation = 0  # Set to 1 for new evaluation; 0 to load results\n",
    "filename_prefix = \"your_prefix\"  # Change this to your desired prefix\n",
    "\n",
    "if run_new_evaluation:\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"LGBMRegressor\": lgb.LGBMRegressor(),\n",
    "        \"XGBoost\": XGBRegressor(),\n",
    "        \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "        \"RandomForestRegressor\": RandomForestRegressor(n_estimators=20),\n",
    "        \"KNeighborsRegressor\": KNeighborsRegressor(n_neighbors=5),\n",
    "        \"SVR\": SVR(kernel='rbf')\n",
    "    }\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        for target in targets:\n",
    "            print(f\"\\nEvaluating {model_name} for {target}...\")\n",
    "            y_train = y_train_dict[target]\n",
    "            \n",
    "            try:\n",
    "                nmse_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "                rmse_scores = np.sqrt(-nmse_scores)\n",
    "                \n",
    "                # Save RMSE scores\n",
    "                joblib.dump(rmse_scores, f'saved_objects/{model_name}_rmse_{target}.pkl')\n",
    "                print(f\"{target} - {model_name} rmse: \", rmse_scores)\n",
    "                print(f\"{target} - Avg. rmse: \", mean(rmse_scores), '\\n')\n",
    "\n",
    "                # Predict using cross-validation\n",
    "                y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "                \n",
    "                # Calculate residuals\n",
    "                residuals = y_train - y_train_pred\n",
    "                \n",
    "                # Plot the residual distribution\n",
    "                plt.figure()\n",
    "                plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "                plt.title(f'{target} - {model_name} Residual Distribution')\n",
    "                plt.xlabel('Residuals')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.grid(True)\n",
    "                plt.savefig(f'models/{filename_prefix}_{target}_{model_name}_residuals_plot.png')  # Save with target name\n",
    "                plt.show()  # Display the plot (useful in Jupyter Notebooks)\n",
    "                plt.close()  # Close the plot to free memory\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name} for {target}: {e}\")\n",
    "else: \n",
    "    # Load RMSE scores from previously saved files\n",
    "    for model_name in [\"LinearRegression\", \"LGBMRegressor\", \"XGBoost\", \"DecisionTreeRegressor\", \"RandomForestRegressor\", \"KNeighborsRegressor\", \"SVR\"]:\n",
    "        for target in targets:\n",
    "            try:\n",
    "                rmse_scores = joblib.load(f'saved_objects/{model_name}_rmse_{target}.pkl')\n",
    "                print(f\"\\n{model_name} rmse for {target}: \", rmse_scores)\n",
    "                print(f\"Avg. rmse for {target}: \", mean(rmse_scores), '\\n')\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found for {model_name} and {target}. Ensure you've run the evaluation first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 9: FINE-TUNING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n____________ Fine-tune models ____________')\n",
    "\n",
    "# Function to print results of grid search\n",
    "def print_search_result(grid_search, model_name=\"\"):\n",
    "    print(\"\\n====== Fine-tune \" + model_name + \" ======\")\n",
    "    print('Best hyperparameter combination: ', grid_search.best_params_)\n",
    "    print('Best rmse: ', np.sqrt(-grid_search.best_score_)) \n",
    "    print('Performance of hyperparameter combinations:')\n",
    "    cv_results = grid_search.cv_results_\n",
    "    for (mean_score, params) in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n",
    "        print('rmse =', np.sqrt(-mean_score), params)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=37)\n",
    "\n",
    "run_new_search = 0  # Set to 1 to run a new search, 0 to load previous results\n",
    "if run_new_search:\n",
    "    # Define a parameter grid for LightGBM\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 500],  # Number of boosting iterations (trees) to be built.\n",
    "        'max_depth': [3, 5, 7, 10],  # Maximum depth of each tree.\n",
    "        'learning_rate': [0.01, 0.05, 0.1],  # Step size shrinkage used to prevent overfitting\n",
    "        'subsample': [0.8, 0.9, 1.0],  # Fraction of samples used for training each tree\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],  # Fraction of features used for training each tree\n",
    "        'reg_alpha': [0, 0.1, 0.5],  # L1 regularization term on weights\n",
    "        'reg_lambda': [0, 0.1, 0.5]  # L2 regularization term on weights\n",
    "    }\n",
    "    \n",
    "    # Set up and run grid search\n",
    "    lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Save grid search results\n",
    "    joblib.dump(grid_search, 'saved_objects/LGBMRegressor_gridsearch.pkl')\n",
    "\n",
    "    # Print search results\n",
    "    print_search_result(grid_search, \"LGBMRegressor\")\n",
    "\n",
    "    # Get best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "else:\n",
    "    # Load previously saved grid search results\n",
    "    grid_search = joblib.load('saved_objects/LGBMRegressor_gridsearch.pkl')\n",
    "    print_search_result(grid_search, model_name=\"LGBMRegressor\")\n",
    "    \n",
    "    # Get best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 10:  PREDICT THE PRICE FOR THE NEXT 7 DAYS USING ALL PAST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_factor = 0.0005\n",
    "# **STEP 10: FINAL PREDICTION AND VISUALIZATION USING LIGHTGBM**\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the trained LightGBM model from step 9\n",
    "lightgbm_model = joblib.load('saved_objects/LGBMRegressor_gridsearch.pkl').best_estimator_\n",
    "\n",
    "# Load the numerical pipeline\n",
    "num_pipeline = joblib.load('models/num_pipeline.pkl')\n",
    "\n",
    "def generate_future_features(last_data, end_date):\n",
    "    days = (end_date - last_data['Date'].iloc[-1]).days\n",
    "    future_dates = pd.date_range(start=last_data['Date'].iloc[-1] + timedelta(days=1), periods=days)\n",
    "    \n",
    "    future_data = pd.DataFrame({'Date': future_dates})\n",
    "    \n",
    "    # Add time-based features\n",
    "    future_data['DayOfWeek'] = future_data['Date'].dt.dayofweek\n",
    "    future_data['Month'] = future_data['Date'].dt.month\n",
    "    future_data['Year'] = future_data['Date'].dt.year\n",
    "    \n",
    "    # For other features, use a simple time series forecast (e.g., moving average) with some noise\n",
    "    for col in last_data.columns:\n",
    "        if col not in ['Date', 'DayOfWeek', 'Month', 'Year'] + targets:\n",
    "            mean_value = last_data[col].rolling(window=30).mean().iloc[-1]\n",
    "            std_value = last_data[col].rolling(window=30).std().iloc[-1]\n",
    "            future_data[col] = np.random.normal(mean_value, std_value + noise_factor, size=len(future_data))\n",
    "    \n",
    "    return future_data\n",
    "\n",
    "# Get the last data point from your original dataset\n",
    "last_data = data.copy()\n",
    "\n",
    "# Add time-based features to historical data\n",
    "last_data['DayOfWeek'] = last_data['Date'].dt.dayofweek\n",
    "last_data['Month'] = last_data['Date'].dt.month\n",
    "last_data['Year'] = last_data['Date'].dt.year\n",
    "\n",
    "# Set the end date for prediction (October 11th, 2024)\n",
    "end_date = datetime(2024, 10, 11)\n",
    "\n",
    "# Generate future features\n",
    "future_data = generate_future_features(last_data, end_date)\n",
    "\n",
    "# Combine historical and future data\n",
    "combined_data = pd.concat([last_data, future_data], ignore_index=True)\n",
    "\n",
    "# Prepare features for the entire dataset\n",
    "X_combined = combined_data.drop(columns=['Date'] + targets)\n",
    "X_combined = num_pipeline.transform(X_combined)\n",
    "\n",
    "# Make predictions for the entire dataset\n",
    "all_predictions = lightgbm_model.predict(X_combined)\n",
    "\n",
    "# Create a DataFrame with all dates and predictions\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Date': combined_data['Date'],\n",
    "    'Actual_Close': combined_data['Close'],\n",
    "    'Predicted_Close': all_predictions\n",
    "})\n",
    "\n",
    "# Visualize the predictions\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(prediction_df['Date'], prediction_df['Actual_Close'], label='Actual Close', color='blue')\n",
    "plt.plot(prediction_df['Date'], prediction_df['Predicted_Close'], label='Predicted Close', color='red', linestyle='--')\n",
    "plt.title('Historical and Predicted Closing Prices up to 11/10/2024')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# New plot for October 7-11, 2024\n",
    "oct_data = prediction_df[(prediction_df['Date'] >= '2024-10-07') & (prediction_df['Date'] <= '2024-10-11')]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(oct_data['Date'], oct_data['Predicted_Close'], marker='o', linestyle='-', color='red')\n",
    "plt.title('Predicted Closing Prices for October 7-11, 2024')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Predicted Close Price')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print out the predicted values for October 7-11, 2024\n",
    "print(\"\\nPredicted Closing Prices for October 7-11, 2024:\")\n",
    "for _, row in oct_data.iterrows():\n",
    "    print(f\"{row['Date'].date()}: {row['Predicted_Close']:.2f}\")\n",
    "\n",
    "# Visualize the predictions with highlighted fluctuations\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot actual close prices\n",
    "plt.plot(prediction_df['Date'], prediction_df['Actual_Close'], label='Actual Close', color='blue', alpha=0.7)\n",
    "\n",
    "# Plot predicted close prices\n",
    "plt.plot(prediction_df['Date'], prediction_df['Predicted_Close'], label='Predicted Close', color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Highlight the difference between actual and predicted\n",
    "plt.fill_between(prediction_df['Date'], prediction_df['Actual_Close'], prediction_df['Predicted_Close'], \n",
    "                 color='gray', alpha=0.3, label='Difference')\n",
    "\n",
    "# Add a rolling mean to show the trend\n",
    "window = 30  # Adjust this value to change the smoothing\n",
    "plt.plot(prediction_df['Date'], prediction_df['Actual_Close'].rolling(window=window).mean(), \n",
    "         label=f'{window}-day Moving Average', color='green', linewidth=2)\n",
    "\n",
    "# Highlight the prediction period\n",
    "prediction_start = prediction_df[prediction_df['Actual_Close'].isna()]['Date'].min()\n",
    "plt.axvline(x=prediction_start, color='purple', linestyle=':', label='Prediction Start')\n",
    "\n",
    "plt.title('Historical and Predicted Closing Prices with Fluctuations', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Price', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add text annotations for max and min points\n",
    "max_point = prediction_df['Actual_Close'].max()\n",
    "min_point = prediction_df['Actual_Close'].min()\n",
    "max_date = prediction_df.loc[prediction_df['Actual_Close'] == max_point, 'Date'].iloc[0]\n",
    "min_date = prediction_df.loc[prediction_df['Actual_Close'] == min_point, 'Date'].iloc[0]\n",
    "\n",
    "plt.annotate(f'Max: {max_point:.2f}', xy=(max_date, max_point), xytext=(10, 10), \n",
    "             textcoords='offset points', ha='left', va='bottom',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "             arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.annotate(f'Min: {min_point:.2f}', xy=(min_date, min_point), xytext=(10, -10), \n",
    "             textcoords='offset points', ha='left', va='top',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "             arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# New plot for October 7-11, 2024 with fluctuations\n",
    "oct_data = prediction_df[(prediction_df['Date'] >= '2024-10-07') & (prediction_df['Date'] <= '2024-10-11')]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=oct_data, x='Date', y='Predicted_Close', marker='o')\n",
    "\n",
    "# Add percentage change annotations\n",
    "for i in range(1, len(oct_data)):\n",
    "    prev_close = oct_data['Predicted_Close'].iloc[i-1]\n",
    "    curr_close = oct_data['Predicted_Close'].iloc[i]\n",
    "    pct_change = (curr_close - prev_close) / prev_close * 100\n",
    "    plt.annotate(f'{pct_change:.2f}%', \n",
    "                 xy=(oct_data['Date'].iloc[i], curr_close),\n",
    "                 xytext=(0, 10 if pct_change >= 0 else -10), \n",
    "                 textcoords='offset points',\n",
    "                 ha='center', va='center',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                 arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.title('Predicted Closing Prices of GBPJPY for October 7-11, 2024 with Daily Changes', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Predicted Close Price', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print out the predicted values and daily changes for October 7-11, 2024\n",
    "print(\"\\nPredicted Closing Prices and Daily Changes of GBPJPY for October 7-11, 2024:\")\n",
    "for i, (_, row) in enumerate(oct_data.iterrows()):\n",
    "    if i == 0:\n",
    "        print(f\"{row['Date'].date()}: {row['Predicted_Close']:.2f}\")\n",
    "    else:\n",
    "        prev_close = oct_data['Predicted_Close'].iloc[i-1]\n",
    "        pct_change = (row['Predicted_Close'] - prev_close) / prev_close * 100\n",
    "        print(f\"{row['Date'].date()}: {row['Predicted_Close']:.2f} (Change: {pct_change:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **STEP 10: FINAL PREDICTION AND VISUALIZATION USING LIGHTGBM**\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the trained LightGBM model from step 9\n",
    "lightgbm_model = joblib.load('saved_objects/LGBMRegressor_gridsearch.pkl').best_estimator_\n",
    "\n",
    "# Load the numerical pipeline\n",
    "num_pipeline = joblib.load('models/num_pipeline.pkl')\n",
    "\n",
    "def generate_future_features(last_data, end_date):\n",
    "    days = (end_date - last_data['Date'].iloc[-1]).days\n",
    "    future_dates = pd.date_range(start=last_data['Date'].iloc[-1] + timedelta(days=1), periods=days)\n",
    "    \n",
    "    future_data = pd.DataFrame({'Date': future_dates})\n",
    "    \n",
    "    # Add time-based features\n",
    "    future_data['DayOfWeek'] = future_data['Date'].dt.dayofweek\n",
    "    future_data['Month'] = future_data['Date'].dt.month\n",
    "    future_data['Year'] = future_data['Date'].dt.year\n",
    "    \n",
    "    # For other features, use a simple time series forecast (e.g., moving average)\n",
    "    for col in last_data.columns:\n",
    "        if col not in ['Date', 'DayOfWeek', 'Month', 'Year'] + targets:\n",
    "            mean_value = last_data[col].rolling(window=30).mean().iloc[-1]\n",
    "            future_data[col] = mean_value  # Removed noise factor\n",
    "    \n",
    "    return future_data\n",
    "\n",
    "# Get the last data point from your original dataset\n",
    "last_data = data.copy()\n",
    "\n",
    "# Add time-based features to historical data\n",
    "last_data['DayOfWeek'] = last_data['Date'].dt.dayofweek\n",
    "last_data['Month'] = last_data['Date'].dt.month\n",
    "last_data['Year'] = last_data['Date'].dt.year\n",
    "\n",
    "# Set the end date for prediction (October 11th, 2024)\n",
    "end_date = datetime(2024, 10, 11)\n",
    "\n",
    "# Generate future features\n",
    "future_data = generate_future_features(last_data, end_date)\n",
    "\n",
    "# Combine historical and future data\n",
    "combined_data = pd.concat([last_data, future_data], ignore_index=True)\n",
    "\n",
    "# Prepare features for the entire dataset\n",
    "X_combined = combined_data.drop(columns=['Date'] + targets)\n",
    "X_combined = num_pipeline.transform(X_combined)\n",
    "\n",
    "# Make predictions for the entire dataset\n",
    "all_predictions = lightgbm_model.predict(X_combined)\n",
    "\n",
    "# Create a DataFrame with all dates and predictions\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Date': combined_data['Date'],\n",
    "    'Actual_Close': combined_data['Close'],\n",
    "    'Predicted_Close': all_predictions\n",
    "})\n",
    "\n",
    "# Visualize the predictions\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(prediction_df['Date'], prediction_df['Actual_Close'], label='Actual Close', color='blue')\n",
    "plt.plot(prediction_df['Date'], prediction_df['Predicted_Close'], label='Predicted Close', color='red', linestyle='--')\n",
    "plt.title('Historical and Predicted Closing Prices up to 11/10/2024')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# New plot for October 7-11, 2024\n",
    "oct_data = prediction_df[(prediction_df['Date'] >= '2024-10-07') & (prediction_df['Date'] <= '2024-10-11')]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(oct_data['Date'], oct_data['Predicted_Close'], marker='o', linestyle='-', color='red')\n",
    "plt.title('Predicted Closing Prices for October 7-11, 2024')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Predicted Close Price')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print out the predicted values for October 7-11, 2024\n",
    "print(\"\\nPredicted Closing Prices for October 7-11, 2024:\")\n",
    "for _, row in oct_data.iterrows():\n",
    "    print(f\"{row['Date'].date()}: {row['Predicted_Close']}\")  # Removed rounding format\n",
    "\n",
    "# Visualize the predictions with highlighted fluctuations\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot actual close prices\n",
    "plt.plot(prediction_df['Date'], prediction_df['Actual_Close'], label='Actual Close', color='blue', alpha=0.7)\n",
    "\n",
    "# Plot predicted close prices\n",
    "plt.plot(prediction_df['Date'], prediction_df['Predicted_Close'], label='Predicted Close', color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Highlight the difference between actual and predicted\n",
    "plt.fill_between(prediction_df['Date'], prediction_df['Actual_Close'], prediction_df['Predicted_Close'], \n",
    "                 color='gray', alpha=0.3, label='Difference')\n",
    "\n",
    "# Add a rolling mean to show the trend\n",
    "window = 30  # Adjust this value to change the smoothing\n",
    "plt.plot(prediction_df['Date'], prediction_df['Actual_Close'].rolling(window=window).mean(), \n",
    "         label=f'{window}-day Moving Average', color='green', linewidth=2)\n",
    "\n",
    "# Highlight the prediction period\n",
    "prediction_start = prediction_df[prediction_df['Actual_Close'].isna()]['Date'].min()\n",
    "plt.axvline(x=prediction_start, color='purple', linestyle=':', label='Prediction Start')\n",
    "\n",
    "plt.title('Historical and Predicted Closing Prices with Fluctuations', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Price', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add text annotations for max and min points\n",
    "max_point = prediction_df['Actual_Close'].max()\n",
    "min_point = prediction_df['Actual_Close'].min()\n",
    "max_date = prediction_df.loc[prediction_df['Actual_Close'] == max_point, 'Date'].iloc[0]\n",
    "min_date = prediction_df.loc[prediction_df['Actual_Close'] == min_point, 'Date'].iloc[0]\n",
    "\n",
    "plt.annotate(f'Max: {max_point}', xy=(max_date, max_point), xytext=(10, 10), \n",
    "             textcoords='offset points', ha='left', va='bottom',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "             arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.annotate(f'Min: {min_point}', xy=(min_date, min_point), xytext=(10, -10), \n",
    "             textcoords='offset points', ha='left', va='top',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "             arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# New plot for October 7-11, 2024 with fluctuations\n",
    "oct_data = prediction_df[(prediction_df['Date'] >= '2024-10-07') & (prediction_df['Date'] <= '2024-10-11')]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=oct_data, x='Date', y='Predicted_Close', marker='o')\n",
    "\n",
    "# Add percentage change annotations\n",
    "for i in range(1, len(oct_data)):\n",
    "    prev_close = oct_data['Predicted_Close'].iloc[i-1]\n",
    "    curr_close = oct_data['Predicted_Close'].iloc[i]\n",
    "    pct_change = (curr_close - prev_close) / prev_close * 100\n",
    "    plt.annotate(f'{pct_change:.2f}%', \n",
    "                 xy=(oct_data['Date'].iloc[i], curr_close),\n",
    "                 xytext=(0, 10 if pct_change >= 0 else -10), \n",
    "                 textcoords='offset points',\n",
    "                 ha='center', va='center',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                 arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.title('Predicted Closing Prices of GBPJPY for October 7-11, 2024 with Daily Changes', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Predicted Close Price', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print out the predicted values and daily changes for October 7-11, 2024\n",
    "print(\"\\nPredicted Closing Prices and Daily Changes of GBPJPY for October 7-11, 2024:\")\n",
    "for i, (_, row) in enumerate(oct_data.iterrows()):\n",
    "    if i == 0:\n",
    "        print(f\"{row['Date'].date()}: {row['Predicted_Close']}\")\n",
    "    else:\n",
    "        prev_close = oct_data['Predicted_Close'].iloc[i-1]\n",
    "        pct_change = (row['Predicted_Close'] - prev_close) / prev_close * 100\n",
    "        print(f\"{row['Date'].date()}: {row['Predicted_Close']} (Change: {pct_change:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
